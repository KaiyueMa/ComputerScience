---
title: "CS422 Section 01"
author: "Kaiyue Ma"
output:
  html_document:
    df_print: paged
---
#2.1
```{r}
library(rpart)
library(caret)
library(rpart.plot)
library(ROCR)
library(lattice)
library(ggplot2)
set.seed(1122)

setwd("/Users/michael/Codes/R/2")
train <- read.csv("adult-train.csv", header=T, sep=",")
test <- read.csv("adult-test.csv", header=T, sep=",")
```

### (a)
```{r}
#bulid the pure set
train[train=="?"]<-NA
pure_train <- na.omit(train)

test[test=="?"]<-NA
pure_test <- na.omit(test)
```

### (b)
```{r}
#bulid the tree
model_income <- rpart(pure_train$income ~., method="class", data = pure_train)
print(model_income)
summary(model_income)
rpart.plot(model_income, fallen.leaves = T, type = 4)
```

#### (i) The top three important predictors in the model are : relationship, marital_status and capital_gain.
#### (ii) The first split is done on predictor Relationship.
#### The predicted class of the first node is '<=50K'.
#### Distribution of observations between the “<=50K” and “>50K” classes at this node is 0.75106926 and 0.24893074.


### (c)
```{r}
#predict
pred <- predict(model_income, pure_test, type="class")
```
```{r}
test_num_pos <- sum(pure_test$income == ">50K")
test_num_neg <- sum(pure_test$income == "<=50K")

tp <- sum(pure_test$income == ">50K" & pred == ">50K")
tn <- sum(pure_test$income == "<=50K" & pred == "<=50K")
table(pred, pure_test$income)

confusionMatrix(pred, pure_test$income)
```

#### (i) The balanced accuracy of the model is 0.726. 
#### (ii) The balanced error rate of the model is 0.274. 
#### (iii) Sensitivity : 0.948     Specificity : 0.504

#### (iv) 
```{r}
# ROC curve

pred.rocr <- predict(model_income, newdata=pure_test, type="prob")[,2]
f.pred <- prediction(pred.rocr, pure_test$income)
f.perf <- performance(f.pred, "tpr", "fpr")
plot(f.perf, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.pred, measure = "auc")
cat(paste("The area under curve (AUC) for this model is ", round(auc@y.values[[1]], 3)))
```
####  The area under curve (AUC) for this model is  0.843

### (d)
```{r}
options("digits"=5)
printcp(model_income)
```
#### The tree won't benefit from a pruning, it will not significant benefit the tree. Because the 4th line's xerror rate is the smallest in the CP table. Look down from the first line the xerror rate it didn't increase. 

### (e)
```{r}
#number
train_num_pos <- sum(pure_train$income == ">50K")
train_num_neg <- sum(pure_train$income == "<=50K")
```
#### (i) The numver of observations are in the class “<=50K” is : 22653
####  The numver of observations are in the class “>50K” is : 7508
#### (ii)
```{r}
#sample
temp_neg <- pure_train[pure_train$income == "<=50K",]
sample_pos <- temp_neg[sample(nrow(temp_neg), train_num_pos, replace = F), ]

#merge
train_pos <- pure_train[which(pure_train$income == ">50K"),]
new_train <- rbind(train_pos, sample_pos)

#random
new_train <- new_train[sample(nrow(new_train), nrow(new_train), replace = F),]
```

#### (iii)
```{r}
new_model_income <- rpart(new_train$income ~., method="class", data = new_train)
```
```{r}
new_pred <- predict(new_model_income, pure_test, type="class")
confusionMatrix(new_pred, pure_test$income)
```
###### (i) The balanced accuracy of the model is 0.809. 
###### (ii) The balanced error rate of the model is 0.191. 
###### (iii) Sensitivity : 0.782     Specificity : 0.835
###### (iv)
```{r}
# ROC curve

pred.rocr <- predict(new_model_income, newdata=pure_test, type="prob")[,2]
f.pred <- prediction(pred.rocr, pure_test$income)
f.perf <- performance(f.pred, "tpr", "fpr")
plot(f.perf, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.pred, measure = "auc")
cat(paste("The area under curve (AUC) for this model is ", round(auc@y.values[[1]], 3)))
```
#####  The area under curve (AUC) for this model is  0.846

### (f)
#### In(C): The Balanced accuracy of the model is 0.726, the Sensitivity is 0.948 and the Specificity is 0.504 and the area under curve (AUC) for this model is  0.843.
#### In(e): The balanced accuracy of the model is 0.809, the Sensitivity is 0.782 and the Specificity is 0.835 and the area under curve (AUC) for this model is  0.846.
#### It shows that the Balanced accuracy and Specificity are increasing but the Sensitivity is decreasing. The area under curve(AUC) has little change.


#2.2
```{r}
library(randomForest)

```
### (a)
```{r}
rf_model <- randomForest(income ~ ., data=pure_train, ntree = 100, importance = T)

plot(rf_model)
```
```{r}
rf_pred <- predict(rf_model, pure_test, type="class")
confusionMatrix(rf_pred, as.factor(pure_test$income))
```
#### (i) The balanced accuracy of the model is ~0.635. 
#### (ii) The accuracy of the model is ~0.818. 
#### (iii) Sensitivity : ~0.997     Specificity : ~0.267.
#### (iv) The numver of observations are in the class “<=50K” is : 22653
####  The numver of observations are in the class “>50K” is : 7508
#### (v) Yes, because there are more data in "<=50k" in train set, thus the sensitivity will be higher compare to the specificity.
#### (vi)
#### For MeanDecreaseAccuracy, capital_gain is the most important variable and native_country is the least importatn variable.
#### For MeanDecreaseGini, capital_gain is the most important variable and race is the least importatn variable.
```{r}

varImpPlot(rf_model)
```
#### (vii) The number of variables tried at each split is 3.
```{r}
print(rf_model)
```

### (b)
```{r}
mtry <- tuneRF(pure_train[,-15], pure_train$income, ntreeTry=500, stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
```
#### (i) The default value of mtry given the number of predictors in the model is 3.
#### (ii) The optimal value of mtry suggested by the method is 2.
#### (iii)
```{r}
new_rf_model <- randomForest(income ~ ., data=pure_train, importance = T, mtry = 2)
new_rf_pred <- predict(new_rf_model, pure_test, type="class")
confusionMatrix(new_rf_pred, as.factor(pure_test$income))
```
###### (1) The balanced accuracy of the model is ~0.645.
###### (2) The accuracy of the model is ~0.824. 
###### (3) Sensitivity : ~0.996     Specificity : ~0.293
###### (5)
###### For MeanDecreaseAccuracy, capital_gain is the most important variable and native_country is the least importatn variable.
###### For MeanDecreaseGini, capital_gain is the most important variable and race is the least importatn variable.
```{r}
varImpPlot(new_rf_model)
```
#### (iv) 
#### In(a): The Accuracy of the model is ~0.819, Balanced accuracy is ~0.635, the Sensitivity is ~0.997 and the Specificity is ~0.267.
#### In(b): The Accuracy of the model is ~0.824, Balanced accuracy is ~0.645, the Sensitivity is ~0.996 and the Specificity is ~0.293.

```{r}
bal_rf_model <- randomForest(income ~ ., data=new_train, importance = T)
bal_rf_pred <- predict(bal_rf_model, pure_test, type="class")
confusionMatrix(bal_rf_pred, as.factor(pure_test$income))
```


#2.3
```{r}
library(arules)
library(arulesViz)
library(viridisLite)

groceries <- read.transactions("groceries.csv", sep = ",")
```
### (i)
```{r}
rules <- apriori(groceries)
```
#### 0 rules I get at this support value.

### (ii)
```{r}
rules_1 <- apriori(groceries, parameter = list(support=0.001))
```
#### Support value is 0.001 when get at least 400 rules.

### (iii)
```{r}
summary(groceries)
sort(itemFrequency(groceries),decreasing = T)
```
#### The most frequently bought item is whole milk. Whole milk's frequency is 0.2555160142.
### (iv)
#### The least frequently bought item is sound storage medium. And its frequency is  0.0001016777.

### (v)
```{r}
inspect(head(rules_1, by = "support",5))
```

### (vi)
```{r}
inspect(head(rules_1, by = "confidence",5))
```

### (vii)
```{r}
inspect(tail(rules_1, by = "support",5))
```

### (viii)
```{r}
inspect(tail(rules_1, by = "confidence",5))
```
