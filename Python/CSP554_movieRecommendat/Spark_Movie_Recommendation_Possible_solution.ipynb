{"cells":[{"cell_type":"markdown","source":["In this notebook, we will use an Alternating Least Squares (ALS) algorithm with Spark APIs to predict the ratings for the movies in [MovieLens small dataset](https://grouplens.org/datasets/movielens/latest/)"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\n%matplotlib inline"],"metadata":{"collapsed":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import os\nimport findspark\nos.environ[\"PYSPARK_PYTHON\"] = \"python3\"\nfindspark.init(\"../../../spark-2.2.1-bin-hadoop2.7\",)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Data ETL and Data Exploration"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession"],"metadata":{"collapsed":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":["spark = (\n    SparkSession.builder\n        .master(\"local[*]\")\n        .appName(\"Spark Movie Recommendation Project\")\n        .getOrCreate()\n)\nsc = spark.sparkContext"],"metadata":{"collapsed":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":["movies = spark.read.load(\"ml-latest-small/movies.csv\", format='csv', header = True)\nratings = spark.read.load(\"ml-latest-small/ratings.csv\", format='csv', header = True)\nlinks = spark.read.load(\"ml-latest-small/links.csv\", format='csv', header = True)\ntags = spark.read.load(\"ml-latest-small/tags.csv\", format='csv', header = True)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":["movies.show(5)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["ratings.show(5)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["print('Distinct values of ratings:')\nprint sorted(ratings.select('rating').distinct().rdd.map(lambda r: r[0]).collect())"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["tmp1 = ratings.groupBy(\"userID\").count().toPandas()['count'].min()\ntmp2 = ratings.groupBy(\"movieId\").count().toPandas()['count'].min()\nprint('For the users that rated movies and the movies that were rated:')\nprint('Minimum number of ratings per user is {}'.format(tmp1))\nprint('Minimum number of ratings per movie is {}'.format(tmp2))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["tmp1 = sum(ratings.groupBy(\"movieId\").count().toPandas()['count'] == 1)\ntmp2 = ratings.select('movieId').distinct().count()\nprint('{} out of {} movies are rated by only one user'.format(tmp1, tmp2))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["links.show(5)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["tags.show(5)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Q1: The number of Users"],"metadata":{}},{"cell_type":"code","source":["ratings.select('userId').union(tags.select('userId')).distinct().count()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### Q2: The number of Movies"],"metadata":{}},{"cell_type":"code","source":["ratings.select('movieId').union(tags.select('movieId')).distinct().count()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Q3:  How many movies are rated by users? List movies not rated before"],"metadata":{}},{"cell_type":"code","source":["num_movies_rated = ratings.select('movieId').distinct().count()\nnum_movies_rated"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["all_movies = ratings.select('movieId').union(tags.select('movieId'))\nrated = ratings.select('movieId')\nnot_rated = all_movies.subtract(rated)\nnot_rated.distinct().show()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### Q4: List Movie Genres"],"metadata":{}},{"cell_type":"code","source":["#movies.select('genres').map(lambda x: x[0].split('|')) ????? does not work\nmovies.select('genres').distinct().show()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Q5: Movie for Each Category"],"metadata":{}},{"cell_type":"code","source":["movies.groupby(\"genres\").count().orderBy(\"count\", ascending=False).show()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["## Prepare Data for Training\nWe will use an RDD-based API from [pyspark.mllib](https://spark.apache.org/docs/2.1.1/mllib-collaborative-filtering.html) to predict the ratings, so let's reload \"ratings.csv\" using ``sc.textFile`` and then convert it to the form of (user, item, rating) tuples."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.recommendation import ALS"],"metadata":{"collapsed":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":["movie_rating = sc.textFile(\"ml-latest-small/ratings.csv\")"],"metadata":{"collapsed":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":["header = movie_rating.take(1)[0]\nrating_data = movie_rating.filter(lambda line: line!=header).map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# check three rows\nrating_data.take(3)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Now we split the data into training/validation/testing sets using a 6/2/2 ratio."],"metadata":{}},{"cell_type":"code","source":["train, validation, test = rating_data.randomSplit([6,2,2],seed = 7856)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":["train.cache()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["validation.cache()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["test.cache()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["## ALS Model Selection and Evaluation\n\nWith the ALS model, we can use a grid search to find the optimal hyperparameters."],"metadata":{}},{"cell_type":"code","source":["def train_ALS(train_data, validation_data, num_iters, reg_param, ranks):\n    min_error = float('inf')\n    best_rank = -1\n    best_regularization = 0\n    best_model = None\n    for rank in ranks:\n        for reg in reg_param:\n            model = ALS.train(train_data, rank, iterations = num_iters, lambda_ = reg)\n            predictions = model.predictAll(validation_data.map(lambda x: (x[0], x[1])))\n            predictions = predictions.map(lambda x: ((x[0], x[1]), x[2]))\n            rate_and_preds = validation_data.map(lambda x: ((int(x[0]), int(x[1])), float(x[2]))).join(predictions)\n            error = math.sqrt(rate_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n            print ('{} latent factors and regularization = {}: validation RMSE is {}'.format(rank, reg, error))\n            if error < min_error:\n                min_error = error\n                best_rank = rank\n                best_regularization = reg\n                best_model = model\n    print ('\\nThe best model has {} latent factors and regularization = {}'.format(best_rank, best_regularization))\n    return best_model"],"metadata":{"collapsed":true},"outputs":[],"execution_count":37},{"cell_type":"code","source":["num_iterations = 10\nranks = [6, 8, 10, 12, 14]\nreg_params = [0.05, 0.1, 0.2, 0.4, 0.8]\n\nimport time\nstart_time = time.time()\n\nfinal_model = train_ALS(train, validation, num_iterations, reg_params, ranks)\n\nprint ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"],"metadata":{"scrolled":true},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["The model with 10 latent factors and lambda = 0.2 yields the best result. Let's plot the learning curves for this model."],"metadata":{}},{"cell_type":"code","source":["# Is there a smarter way to plot the learning curves without re-training\n# the model repeatedly...?\ndef plot_learning_curve(iter_array, train_data, validation_data, reg, rank):\n    \n    train_rmse = []\n    valid_rmse = []\n    for num_iters in iter_array:\n        \n        model = ALS.train(train_data, rank, iterations = num_iters, lambda_ = reg)\n        \n        predictions = model.predictAll(validation_data.map(lambda x: (x[0], x[1])))\n        predictions = predictions.map(lambda x: ((x[0], x[1]), x[2]))\n        rate_and_preds = validation_data.map(lambda x: ((int(x[0]), int(x[1])), float(x[2]))).join(predictions)\n        error = math.sqrt(rate_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n        valid_rmse.append(error)\n        \n        predictions = model.predictAll(train_data.map(lambda x: (x[0], x[1])))\n        predictions = predictions.map(lambda x: ((x[0], x[1]), x[2]))\n        rate_and_preds = train_data.map(lambda x: ((int(x[0]), int(x[1])), float(x[2]))).join(predictions)\n        error = math.sqrt(rate_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n        train_rmse.append(error)\n        \n    plt.plot(iter_array, train_rmse, label='Training', linewidth=5)\n    plt.plot(iter_array, valid_rmse, label='Validation', linewidth=5)\n    plt.xticks(range(0, max(iter_array) + 1, 2), fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlabel('iterations', fontsize=30)\n    plt.ylabel('RMSE', fontsize=30)\n    plt.legend(loc='best', fontsize=20) \n    plt.show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":40},{"cell_type":"code","source":["iter_array = [1, 2, 5, 10]\nplot_learning_curve(iter_array, train, validation, 0.2, 10)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["And finally, let's check the testing error."],"metadata":{}},{"cell_type":"code","source":["predictions = final_model.predictAll(test.map(lambda x: (x[0], x[1]))) \npredictions = predictions.map(lambda x: ((x[0], x[1]), x[2]))\nrates_and_preds = test.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\nerror = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\nprint ('For testing data the RMSE is %s' % (error))"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["This is slightly better than the validation error (0.936)."],"metadata":{"collapsed":true}},{"cell_type":"code","source":[""],"metadata":{"collapsed":true},"outputs":[],"execution_count":45}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.14","nbconvert_exporter":"python","file_extension":".py"},"name":"Spark_Movie_Recommendation_Possible_solution (1)","notebookId":1351749028307702},"nbformat":4,"nbformat_minor":0}
